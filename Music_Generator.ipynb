{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from music21 import *"
      ],
      "metadata": {
        "id": "5_SGZNWnlWLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRV2daIjd0rA"
      },
      "outputs": [],
      "source": [
        "\n",
        "def read_midi_file(file):\n",
        "\n",
        "    print(\"Loading Music File:\",file)\n",
        "\n",
        "    notes=[]\n",
        "    notes_to_parse = None\n",
        "\n",
        "\n",
        "    midi_file = converter.parse(file)\n",
        "\n",
        "\n",
        "    s2 = instrument.partitionByInstrument(midi_file)\n",
        "\n",
        "\n",
        "    for part in s2.parts:\n",
        "\n",
        "\n",
        "        if 'Piano' in str(part):\n",
        "\n",
        "            notes_to_parse = part.recurse()\n",
        "\n",
        "\n",
        "            for element in notes_to_parse:\n",
        "\n",
        "                #note\n",
        "                if isinstance(element, note.Note):\n",
        "                    notes.append(str(element.pitch))\n",
        "\n",
        "                #chord\n",
        "                elif isinstance(element, chord.Chord):\n",
        "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
        "\n",
        "    return np.array(notes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# path='/content/drive/MyDrive/MIDI Samples/'\n",
        "path = '/content/'\n",
        "\n",
        "\n",
        "files_to_read=[i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
        "\n",
        "notes_array = np.array([read_midi_file(path+i) for i in files_to_read])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6Zx8NtJlHO5",
        "outputId": "98b7511e-8b7a-401e-f78c-3f3513fc7cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Music File: /content/Colors_of_the_Wind.1.mid\n",
            "Loading Music File: /content/Save_The_Best_For_Last.4.mid\n",
            "Loading Music File: /content/Save_The_Best_For_Last.2.mid\n",
            "Loading Music File: /content/Save_The_Best_For_Last.3.mid\n",
            "Loading Music File: /content/Save_The_Best_For_Last.mid\n",
            "Loading Music File: /content/Colors_of_the_Wind.mid\n",
            "Loading Music File: /content/Save_The_Best_For_Last.1.mid\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-6a104f64eb55>:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  notes_array = np.array([read_midi_file(path+i) for i in files_to_read])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "notes_ = [element for note_ in notes_array for element in note_]\n",
        "\n",
        "\n",
        "unique_notes = list(set(notes_))\n",
        "print(len(unique_notes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-TRfIH-laCx",
        "outputId": "4e66b617-b907-4a95-872e-a67fc2f41f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "freq = dict(Counter(notes_))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nom=[count for _,count in freq.items()]\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "\n",
        "plt.hist(nom)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "SihahADLlaqT",
        "outputId": "7d290b4b-eb0b-444e-dc69-70cc891fe007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([140.,   8.,   7.,   5.,   2.,   1.,   1.,   0.,   1.,   1.]),\n",
              " array([  1. ,  53.4, 105.8, 158.2, 210.6, 263. , 315.4, 367.8, 420.2,\n",
              "        472.6, 525. ]),\n",
              " <BarContainer object of 10 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "image/png": {
              "width": 443,
              "height": 428
            }
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frequent_notes = [note_ for note_, count in freq.items() if count>=50]\n",
        "print(len(frequent_notes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nf7NFp3WmDe6",
        "outputId": "bfd675c2-c642-4363-e345-694000411147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_music=[]\n",
        "\n",
        "for notes in notes_array:\n",
        "    temp=[]\n",
        "    for note_ in notes:\n",
        "        if note_ in frequent_notes:\n",
        "            temp.append(note_)\n",
        "    new_music.append(temp)\n",
        "\n",
        "new_music = np.array(new_music)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qb6rVFG7ljAD",
        "outputId": "4f7f7779-8ebc-4eaf-f528-256a036b7982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-2fa7923ce1b3>:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  new_music = np.array(new_music)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_of_timesteps = 128\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for note_ in new_music:\n",
        "    for i in range(0, len(note_) - no_of_timesteps, 1):\n",
        "\n",
        "        input_ = note_[i:i + no_of_timesteps]\n",
        "        output = note_[i + no_of_timesteps]\n",
        "\n",
        "        x.append(input_)\n",
        "        y.append(output)\n",
        "\n",
        "x=np.array(x)\n",
        "y=np.array(y)"
      ],
      "metadata": {
        "id": "9P1fPg5OljpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_x = list(set(x.ravel()))\n",
        "x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))"
      ],
      "metadata": {
        "id": "XnZISfNpmJzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x_seq=[]\n",
        "for i in x:\n",
        "    temp=[]\n",
        "    for j in i:\n",
        "\n",
        "        temp.append(x_note_to_int[j])\n",
        "    x_seq.append(temp)\n",
        "\n",
        "x_seq = np.array(x_seq)"
      ],
      "metadata": {
        "id": "VuIFfEmIlm0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_y = list(set(y))\n",
        "y_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_y))\n",
        "y_seq=np.array([y_note_to_int[i] for i in y])"
      ],
      "metadata": {
        "id": "Ct3CkGx3mO21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(x_seq,y_seq,test_size=0.2,random_state=0)"
      ],
      "metadata": {
        "id": "0coSFSv3mW_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm():\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(128,return_sequences=True))\n",
        "  model.add(LSTM(128))\n",
        "  model.add(Dense(256))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(n_vocab))\n",
        "  model.add(Activation('softmax'))\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "  return model"
      ],
      "metadata": {
        "id": "mxhSGWc6lqHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras.callbacks import *\n",
        "import keras.backend as K\n",
        "\n",
        "K.clear_session()\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "model.add(Embedding(len(unique_x), 100, input_length=128,trainable=True))\n",
        "\n",
        "model.add(Conv1D(64,3, padding='causal',activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(MaxPool1D(2))\n",
        "\n",
        "model.add(Conv1D(128,3,activation='relu',dilation_rate=2,padding='causal'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(MaxPool1D(2))\n",
        "\n",
        "model.add(Conv1D(256,3,activation='relu',dilation_rate=4,padding='causal'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(MaxPool1D(2))\n",
        "\n",
        "#model.add(Conv1D(256,5,activation='relu'))\n",
        "model.add(GlobalMaxPool1D())\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(len(unique_y), activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5SOtJJblq69",
        "outputId": "de8f796c-b84f-4208-e5e2-394251ead694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 128, 100)          2700      \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 128, 64)           19264     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128, 64)           0         \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 64, 64)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 64, 128)           24704     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64, 128)           0         \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 32, 128)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 32, 256)           98560     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 32, 256)           0         \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPooling  (None, 16, 256)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 256)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 27)                6939      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 217,959\n",
            "Trainable params: 217,959\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mc=ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True,verbose=1)\n",
        "history = model.fit(np.array(x_tr),np.array(y_tr),batch_size=128,epochs=300, validation_data=(np.array(x_val),np.array(y_val)),verbose=1, callbacks=[mc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE35_75CmqlR",
        "outputId": "abc7f7ac-bad3-473d-a03a-cd5ebdae1bf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "24/24 [==============================] - ETA: 0s - loss: 3.1345\n",
            "Epoch 1: val_loss improved from inf to 3.11256, saving model to best_model.h5\n",
            "24/24 [==============================] - 14s 56ms/step - loss: 3.1345 - val_loss: 3.1126\n",
            "Epoch 2/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 3.0619\n",
            "Epoch 2: val_loss improved from 3.11256 to 3.11033, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 3.0666 - val_loss: 3.1103\n",
            "Epoch 3/300\n",
            "24/24 [==============================] - ETA: 0s - loss: 3.0531\n",
            "Epoch 3: val_loss improved from 3.11033 to 3.10939, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 3.0531 - val_loss: 3.1094\n",
            "Epoch 4/300\n",
            "19/24 [======================>.......] - ETA: 0s - loss: 3.0476\n",
            "Epoch 4: val_loss improved from 3.10939 to 3.09139, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 3.0460 - val_loss: 3.0914\n",
            "Epoch 5/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 3.0211\n",
            "Epoch 5: val_loss improved from 3.09139 to 3.07000, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 3.0216 - val_loss: 3.0700\n",
            "Epoch 6/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 3.0010\n",
            "Epoch 6: val_loss improved from 3.07000 to 3.03134, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 2.9954 - val_loss: 3.0313\n",
            "Epoch 7/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 2.9586\n",
            "Epoch 7: val_loss improved from 3.03134 to 3.00234, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 2.9614 - val_loss: 3.0023\n",
            "Epoch 8/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 2.9106\n",
            "Epoch 8: val_loss improved from 3.00234 to 2.95262, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 2.9079 - val_loss: 2.9526\n",
            "Epoch 9/300\n",
            "17/24 [====================>.........] - ETA: 0s - loss: 2.8559\n",
            "Epoch 9: val_loss improved from 2.95262 to 2.86989, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 2.8390 - val_loss: 2.8699\n",
            "Epoch 10/300\n",
            "24/24 [==============================] - ETA: 0s - loss: 2.7553\n",
            "Epoch 10: val_loss improved from 2.86989 to 2.79345, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 2.7553 - val_loss: 2.7935\n",
            "Epoch 11/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 2.6375\n",
            "Epoch 11: val_loss improved from 2.79345 to 2.74277, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 2.6263 - val_loss: 2.7428\n",
            "Epoch 12/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 2.5561\n",
            "Epoch 12: val_loss improved from 2.74277 to 2.66140, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 2.5535 - val_loss: 2.6614\n",
            "Epoch 13/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 2.4308\n",
            "Epoch 13: val_loss improved from 2.66140 to 2.56758, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 2.4300 - val_loss: 2.5676\n",
            "Epoch 14/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 2.3239\n",
            "Epoch 14: val_loss improved from 2.56758 to 2.47791, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 2.3245 - val_loss: 2.4779\n",
            "Epoch 15/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 2.2356\n",
            "Epoch 15: val_loss improved from 2.47791 to 2.37977, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 2.2283 - val_loss: 2.3798\n",
            "Epoch 16/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 2.1195\n",
            "Epoch 16: val_loss improved from 2.37977 to 2.27884, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 16ms/step - loss: 2.1172 - val_loss: 2.2788\n",
            "Epoch 17/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 2.0031\n",
            "Epoch 17: val_loss improved from 2.27884 to 2.22381, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 17ms/step - loss: 2.0085 - val_loss: 2.2238\n",
            "Epoch 18/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.9220\n",
            "Epoch 18: val_loss improved from 2.22381 to 2.08515, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 16ms/step - loss: 1.9220 - val_loss: 2.0852\n",
            "Epoch 19/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.7970\n",
            "Epoch 19: val_loss improved from 2.08515 to 1.95245, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 14ms/step - loss: 1.7921 - val_loss: 1.9524\n",
            "Epoch 20/300\n",
            "24/24 [==============================] - ETA: 0s - loss: 1.6897\n",
            "Epoch 20: val_loss improved from 1.95245 to 1.87383, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 14ms/step - loss: 1.6897 - val_loss: 1.8738\n",
            "Epoch 21/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.6372\n",
            "Epoch 21: val_loss improved from 1.87383 to 1.80507, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 15ms/step - loss: 1.6351 - val_loss: 1.8051\n",
            "Epoch 22/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 1.5323\n",
            "Epoch 22: val_loss improved from 1.80507 to 1.69018, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 1.5340 - val_loss: 1.6902\n",
            "Epoch 23/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 1.4582\n",
            "Epoch 23: val_loss improved from 1.69018 to 1.58103, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 1.4615 - val_loss: 1.5810\n",
            "Epoch 24/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 1.3634\n",
            "Epoch 24: val_loss improved from 1.58103 to 1.49330, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 1.3624 - val_loss: 1.4933\n",
            "Epoch 25/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.3326\n",
            "Epoch 25: val_loss improved from 1.49330 to 1.43199, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 1.3337 - val_loss: 1.4320\n",
            "Epoch 26/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.2479\n",
            "Epoch 26: val_loss improved from 1.43199 to 1.37501, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 1.2535 - val_loss: 1.3750\n",
            "Epoch 27/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.1845\n",
            "Epoch 27: val_loss improved from 1.37501 to 1.36389, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 1.1844 - val_loss: 1.3639\n",
            "Epoch 28/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 1.1507\n",
            "Epoch 28: val_loss improved from 1.36389 to 1.30336, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 1.1435 - val_loss: 1.3034\n",
            "Epoch 29/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 1.0984\n",
            "Epoch 29: val_loss improved from 1.30336 to 1.29419, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 1.1006 - val_loss: 1.2942\n",
            "Epoch 30/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 1.0503\n",
            "Epoch 30: val_loss improved from 1.29419 to 1.21328, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 1.0529 - val_loss: 1.2133\n",
            "Epoch 31/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.9795\n",
            "Epoch 31: val_loss improved from 1.21328 to 1.19422, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.9788 - val_loss: 1.1942\n",
            "Epoch 32/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.9619\n",
            "Epoch 32: val_loss improved from 1.19422 to 1.17423, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.9622 - val_loss: 1.1742\n",
            "Epoch 33/300\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.9426\n",
            "Epoch 33: val_loss improved from 1.17423 to 1.15202, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.9426 - val_loss: 1.1520\n",
            "Epoch 34/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.8656\n",
            "Epoch 34: val_loss improved from 1.15202 to 1.12729, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.8674 - val_loss: 1.1273\n",
            "Epoch 35/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.8990\n",
            "Epoch 35: val_loss improved from 1.12729 to 1.11514, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.9004 - val_loss: 1.1151\n",
            "Epoch 36/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.8321\n",
            "Epoch 36: val_loss improved from 1.11514 to 1.09583, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.8278 - val_loss: 1.0958\n",
            "Epoch 37/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.7914\n",
            "Epoch 37: val_loss improved from 1.09583 to 1.05748, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7958 - val_loss: 1.0575\n",
            "Epoch 38/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.7685\n",
            "Epoch 38: val_loss did not improve from 1.05748\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.7715 - val_loss: 1.0827\n",
            "Epoch 39/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.7496\n",
            "Epoch 39: val_loss did not improve from 1.05748\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.7490 - val_loss: 1.0688\n",
            "Epoch 40/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.7199\n",
            "Epoch 40: val_loss improved from 1.05748 to 1.05572, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.7242 - val_loss: 1.0557\n",
            "Epoch 41/300\n",
            "19/24 [======================>.......] - ETA: 0s - loss: 0.7402\n",
            "Epoch 41: val_loss improved from 1.05572 to 1.02904, saving model to best_model.h5\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.7448 - val_loss: 1.0290\n",
            "Epoch 42/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.6938\n",
            "Epoch 42: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.6998 - val_loss: 1.0348\n",
            "Epoch 43/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.6778\n",
            "Epoch 43: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.6760 - val_loss: 1.0476\n",
            "Epoch 44/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.6470\n",
            "Epoch 44: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.6485 - val_loss: 1.0414\n",
            "Epoch 45/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.6061\n",
            "Epoch 45: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.6140 - val_loss: 1.0718\n",
            "Epoch 46/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.6074\n",
            "Epoch 46: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.6085 - val_loss: 1.0472\n",
            "Epoch 47/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.6117\n",
            "Epoch 47: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.6170 - val_loss: 1.0397\n",
            "Epoch 48/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.5610\n",
            "Epoch 48: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.5678 - val_loss: 1.0505\n",
            "Epoch 49/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.5648\n",
            "Epoch 49: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.5634 - val_loss: 1.0697\n",
            "Epoch 50/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.5291\n",
            "Epoch 50: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.5353 - val_loss: 1.0580\n",
            "Epoch 51/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.5397\n",
            "Epoch 51: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.5359 - val_loss: 1.0512\n",
            "Epoch 52/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.5102\n",
            "Epoch 52: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.5159 - val_loss: 1.0645\n",
            "Epoch 53/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.4834\n",
            "Epoch 53: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.4829 - val_loss: 1.0835\n",
            "Epoch 54/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.4915\n",
            "Epoch 54: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.4950 - val_loss: 1.0965\n",
            "Epoch 55/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.4871\n",
            "Epoch 55: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.4850 - val_loss: 1.0755\n",
            "Epoch 56/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.4589\n",
            "Epoch 56: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.4602 - val_loss: 1.0792\n",
            "Epoch 57/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.4379\n",
            "Epoch 57: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.4405 - val_loss: 1.1144\n",
            "Epoch 58/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.4426\n",
            "Epoch 58: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.4410 - val_loss: 1.1091\n",
            "Epoch 59/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.4145\n",
            "Epoch 59: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.4106 - val_loss: 1.1013\n",
            "Epoch 60/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.4156\n",
            "Epoch 60: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.4238 - val_loss: 1.1046\n",
            "Epoch 61/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.3976\n",
            "Epoch 61: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.3986 - val_loss: 1.1462\n",
            "Epoch 62/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.3614\n",
            "Epoch 62: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.3681 - val_loss: 1.1518\n",
            "Epoch 63/300\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3789\n",
            "Epoch 63: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 14ms/step - loss: 0.3789 - val_loss: 1.1746\n",
            "Epoch 64/300\n",
            "19/24 [======================>.......] - ETA: 0s - loss: 0.3500\n",
            "Epoch 64: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 13ms/step - loss: 0.3695 - val_loss: 1.1371\n",
            "Epoch 65/300\n",
            "19/24 [======================>.......] - ETA: 0s - loss: 0.3705\n",
            "Epoch 65: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.3704 - val_loss: 1.1631\n",
            "Epoch 66/300\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3501\n",
            "Epoch 66: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 19ms/step - loss: 0.3501 - val_loss: 1.1504\n",
            "Epoch 67/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.3307\n",
            "Epoch 67: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 1s 27ms/step - loss: 0.3319 - val_loss: 1.1848\n",
            "Epoch 68/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.3412\n",
            "Epoch 68: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 18ms/step - loss: 0.3556 - val_loss: 1.1918\n",
            "Epoch 69/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.3389\n",
            "Epoch 69: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.3437 - val_loss: 1.1983\n",
            "Epoch 70/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.3372\n",
            "Epoch 70: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 16ms/step - loss: 0.3399 - val_loss: 1.1809\n",
            "Epoch 71/300\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.3105\n",
            "Epoch 71: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 16ms/step - loss: 0.3105 - val_loss: 1.1905\n",
            "Epoch 72/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.3175\n",
            "Epoch 72: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.3217 - val_loss: 1.2075\n",
            "Epoch 73/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.3299\n",
            "Epoch 73: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.3295 - val_loss: 1.2163\n",
            "Epoch 74/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.2911\n",
            "Epoch 74: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.2917 - val_loss: 1.2628\n",
            "Epoch 75/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.2753\n",
            "Epoch 75: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.2733 - val_loss: 1.2316\n",
            "Epoch 76/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.2819\n",
            "Epoch 76: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.2817 - val_loss: 1.2310\n",
            "Epoch 77/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.2730\n",
            "Epoch 77: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.2715 - val_loss: 1.2553\n",
            "Epoch 78/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.2734\n",
            "Epoch 78: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.2713 - val_loss: 1.2350\n",
            "Epoch 79/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.2423\n",
            "Epoch 79: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.2448 - val_loss: 1.2816\n",
            "Epoch 80/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.2727\n",
            "Epoch 80: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.2754 - val_loss: 1.2555\n",
            "Epoch 81/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.2764\n",
            "Epoch 81: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.2743 - val_loss: 1.2764\n",
            "Epoch 82/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.2538\n",
            "Epoch 82: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.2524 - val_loss: 1.2771\n",
            "Epoch 83/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.2448\n",
            "Epoch 83: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.2411 - val_loss: 1.2750\n",
            "Epoch 84/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.2332\n",
            "Epoch 84: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.2401 - val_loss: 1.3283\n",
            "Epoch 85/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.2426\n",
            "Epoch 85: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.2440 - val_loss: 1.3584\n",
            "Epoch 86/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.2264\n",
            "Epoch 86: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.2258 - val_loss: 1.3300\n",
            "Epoch 87/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.2285\n",
            "Epoch 87: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.2269 - val_loss: 1.3424\n",
            "Epoch 88/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.2134\n",
            "Epoch 88: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.2164 - val_loss: 1.3635\n",
            "Epoch 89/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.2244\n",
            "Epoch 89: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.2244 - val_loss: 1.3366\n",
            "Epoch 90/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.2221\n",
            "Epoch 90: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.2219 - val_loss: 1.3703\n",
            "Epoch 91/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.2241\n",
            "Epoch 91: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.2245 - val_loss: 1.3657\n",
            "Epoch 92/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.2134\n",
            "Epoch 92: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.2138 - val_loss: 1.3714\n",
            "Epoch 93/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.1998\n",
            "Epoch 93: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1998 - val_loss: 1.3896\n",
            "Epoch 94/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.2157\n",
            "Epoch 94: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.2196 - val_loss: 1.3463\n",
            "Epoch 95/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.2029\n",
            "Epoch 95: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.2030 - val_loss: 1.3527\n",
            "Epoch 96/300\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.1952\n",
            "Epoch 96: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1952 - val_loss: 1.3934\n",
            "Epoch 97/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.1889\n",
            "Epoch 97: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.1909 - val_loss: 1.3998\n",
            "Epoch 98/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1910\n",
            "Epoch 98: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.1910 - val_loss: 1.4134\n",
            "Epoch 99/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.2082\n",
            "Epoch 99: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.2142 - val_loss: 1.3932\n",
            "Epoch 100/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1862\n",
            "Epoch 100: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1893 - val_loss: 1.4320\n",
            "Epoch 101/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.1705\n",
            "Epoch 101: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.1714 - val_loss: 1.4030\n",
            "Epoch 102/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.1815\n",
            "Epoch 102: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.1836 - val_loss: 1.4340\n",
            "Epoch 103/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.1965\n",
            "Epoch 103: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.1899 - val_loss: 1.4630\n",
            "Epoch 104/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1711\n",
            "Epoch 104: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.1731 - val_loss: 1.4309\n",
            "Epoch 105/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1739\n",
            "Epoch 105: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1768 - val_loss: 1.4874\n",
            "Epoch 106/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1806\n",
            "Epoch 106: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.1826 - val_loss: 1.4532\n",
            "Epoch 107/300\n",
            "19/24 [======================>.......] - ETA: 0s - loss: 0.1772\n",
            "Epoch 107: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 13ms/step - loss: 0.1799 - val_loss: 1.4556\n",
            "Epoch 108/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1456\n",
            "Epoch 108: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.1445 - val_loss: 1.4735\n",
            "Epoch 109/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1602\n",
            "Epoch 109: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.1666 - val_loss: 1.4871\n",
            "Epoch 110/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1663\n",
            "Epoch 110: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 21ms/step - loss: 0.1717 - val_loss: 1.4704\n",
            "Epoch 111/300\n",
            "19/24 [======================>.......] - ETA: 0s - loss: 0.1675\n",
            "Epoch 111: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.1727 - val_loss: 1.5086\n",
            "Epoch 112/300\n",
            "19/24 [======================>.......] - ETA: 0s - loss: 0.1517\n",
            "Epoch 112: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.1622 - val_loss: 1.4900\n",
            "Epoch 113/300\n",
            "19/24 [======================>.......] - ETA: 0s - loss: 0.1617\n",
            "Epoch 113: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.1593 - val_loss: 1.5016\n",
            "Epoch 114/300\n",
            "19/24 [======================>.......] - ETA: 0s - loss: 0.1592\n",
            "Epoch 114: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 14ms/step - loss: 0.1592 - val_loss: 1.5156\n",
            "Epoch 115/300\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.1621\n",
            "Epoch 115: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 14ms/step - loss: 0.1621 - val_loss: 1.4731\n",
            "Epoch 116/300\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.1471\n",
            "Epoch 116: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 13ms/step - loss: 0.1471 - val_loss: 1.4906\n",
            "Epoch 117/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1492\n",
            "Epoch 117: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1486 - val_loss: 1.5243\n",
            "Epoch 118/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1379\n",
            "Epoch 118: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1401 - val_loss: 1.5161\n",
            "Epoch 119/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1616\n",
            "Epoch 119: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1562 - val_loss: 1.5382\n",
            "Epoch 120/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1335\n",
            "Epoch 120: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1327 - val_loss: 1.5464\n",
            "Epoch 121/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.1237\n",
            "Epoch 121: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1272 - val_loss: 1.5659\n",
            "Epoch 122/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.1343\n",
            "Epoch 122: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1317 - val_loss: 1.5437\n",
            "Epoch 123/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1326\n",
            "Epoch 123: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1309 - val_loss: 1.5307\n",
            "Epoch 124/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1284\n",
            "Epoch 124: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1258 - val_loss: 1.5551\n",
            "Epoch 125/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.1462\n",
            "Epoch 125: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1448 - val_loss: 1.5433\n",
            "Epoch 126/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1338\n",
            "Epoch 126: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1323 - val_loss: 1.5532\n",
            "Epoch 127/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1349\n",
            "Epoch 127: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1348 - val_loss: 1.5696\n",
            "Epoch 128/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.1166\n",
            "Epoch 128: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1166 - val_loss: 1.5387\n",
            "Epoch 129/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1229\n",
            "Epoch 129: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1189 - val_loss: 1.5717\n",
            "Epoch 130/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.1160\n",
            "Epoch 130: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1130 - val_loss: 1.5551\n",
            "Epoch 131/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1239\n",
            "Epoch 131: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1223 - val_loss: 1.5901\n",
            "Epoch 132/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1113\n",
            "Epoch 132: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1125 - val_loss: 1.6684\n",
            "Epoch 133/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1224\n",
            "Epoch 133: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.1242 - val_loss: 1.6219\n",
            "Epoch 134/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.1230\n",
            "Epoch 134: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.1228 - val_loss: 1.5792\n",
            "Epoch 135/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1380\n",
            "Epoch 135: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1342 - val_loss: 1.6327\n",
            "Epoch 136/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.1402\n",
            "Epoch 136: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.1427 - val_loss: 1.6277\n",
            "Epoch 137/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1226\n",
            "Epoch 137: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1268 - val_loss: 1.6509\n",
            "Epoch 138/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1222\n",
            "Epoch 138: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.1255 - val_loss: 1.6048\n",
            "Epoch 139/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.1367\n",
            "Epoch 139: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.1363 - val_loss: 1.6281\n",
            "Epoch 140/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.1089\n",
            "Epoch 140: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.1108 - val_loss: 1.6694\n",
            "Epoch 141/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1139\n",
            "Epoch 141: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1145 - val_loss: 1.6726\n",
            "Epoch 142/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1013\n",
            "Epoch 142: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.1033 - val_loss: 1.6328\n",
            "Epoch 143/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1202\n",
            "Epoch 143: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.1222 - val_loss: 1.6343\n",
            "Epoch 144/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1088\n",
            "Epoch 144: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1104 - val_loss: 1.6125\n",
            "Epoch 145/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1119\n",
            "Epoch 145: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1110 - val_loss: 1.6656\n",
            "Epoch 146/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1104\n",
            "Epoch 146: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1075 - val_loss: 1.6816\n",
            "Epoch 147/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1108\n",
            "Epoch 147: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1110 - val_loss: 1.7159\n",
            "Epoch 148/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1190\n",
            "Epoch 148: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1276 - val_loss: 1.6442\n",
            "Epoch 149/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1187\n",
            "Epoch 149: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1227 - val_loss: 1.6577\n",
            "Epoch 150/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0987\n",
            "Epoch 150: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.0989 - val_loss: 1.6798\n",
            "Epoch 151/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0962\n",
            "Epoch 151: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0982 - val_loss: 1.7266\n",
            "Epoch 152/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1072\n",
            "Epoch 152: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1031 - val_loss: 1.7211\n",
            "Epoch 153/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0923\n",
            "Epoch 153: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.0924 - val_loss: 1.7563\n",
            "Epoch 154/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0899\n",
            "Epoch 154: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.0969 - val_loss: 1.7645\n",
            "Epoch 155/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0974\n",
            "Epoch 155: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0987 - val_loss: 1.7239\n",
            "Epoch 156/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1036\n",
            "Epoch 156: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1034 - val_loss: 1.7366\n",
            "Epoch 157/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0977\n",
            "Epoch 157: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0977 - val_loss: 1.7305\n",
            "Epoch 158/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0998\n",
            "Epoch 158: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.0988 - val_loss: 1.6658\n",
            "Epoch 159/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1060\n",
            "Epoch 159: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1043 - val_loss: 1.6517\n",
            "Epoch 160/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1193\n",
            "Epoch 160: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 14ms/step - loss: 0.1191 - val_loss: 1.6426\n",
            "Epoch 161/300\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.1081\n",
            "Epoch 161: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 14ms/step - loss: 0.1081 - val_loss: 1.6687\n",
            "Epoch 162/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.0935\n",
            "Epoch 162: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.0978 - val_loss: 1.6707\n",
            "Epoch 163/300\n",
            "19/24 [======================>.......] - ETA: 0s - loss: 0.1045\n",
            "Epoch 163: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.1012 - val_loss: 1.6103\n",
            "Epoch 164/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.0983\n",
            "Epoch 164: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 14ms/step - loss: 0.0988 - val_loss: 1.7018\n",
            "Epoch 165/300\n",
            "19/24 [======================>.......] - ETA: 0s - loss: 0.1053\n",
            "Epoch 165: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 13ms/step - loss: 0.1022 - val_loss: 1.7286\n",
            "Epoch 166/300\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.0959\n",
            "Epoch 166: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 13ms/step - loss: 0.0959 - val_loss: 1.7070\n",
            "Epoch 167/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.0974\n",
            "Epoch 167: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.0987 - val_loss: 1.7275\n",
            "Epoch 168/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0850\n",
            "Epoch 168: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0857 - val_loss: 1.7396\n",
            "Epoch 169/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0891\n",
            "Epoch 169: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.0933 - val_loss: 1.7665\n",
            "Epoch 170/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0965\n",
            "Epoch 170: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0974 - val_loss: 1.7373\n",
            "Epoch 171/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.1033\n",
            "Epoch 171: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.1109 - val_loss: 1.7877\n",
            "Epoch 172/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0955\n",
            "Epoch 172: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0954 - val_loss: 1.6596\n",
            "Epoch 173/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0847\n",
            "Epoch 173: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0854 - val_loss: 1.7123\n",
            "Epoch 174/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0805\n",
            "Epoch 174: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.0821 - val_loss: 1.7007\n",
            "Epoch 175/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0824\n",
            "Epoch 175: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0837 - val_loss: 1.7239\n",
            "Epoch 176/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.1087\n",
            "Epoch 176: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.1043 - val_loss: 1.7422\n",
            "Epoch 177/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0790\n",
            "Epoch 177: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0810 - val_loss: 1.7838\n",
            "Epoch 178/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.1010\n",
            "Epoch 178: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0986 - val_loss: 1.7875\n",
            "Epoch 179/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0901\n",
            "Epoch 179: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0946 - val_loss: 1.7642\n",
            "Epoch 180/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0812\n",
            "Epoch 180: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.0781 - val_loss: 1.7698\n",
            "Epoch 181/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0957\n",
            "Epoch 181: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.0969 - val_loss: 1.7943\n",
            "Epoch 182/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0900\n",
            "Epoch 182: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0918 - val_loss: 1.7715\n",
            "Epoch 183/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0991\n",
            "Epoch 183: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.0959 - val_loss: 1.7291\n",
            "Epoch 184/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0727\n",
            "Epoch 184: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0728 - val_loss: 1.7972\n",
            "Epoch 185/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0786\n",
            "Epoch 185: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0806 - val_loss: 1.8517\n",
            "Epoch 186/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0836\n",
            "Epoch 186: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0843 - val_loss: 1.8092\n",
            "Epoch 187/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0873\n",
            "Epoch 187: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0907 - val_loss: 1.7483\n",
            "Epoch 188/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0865\n",
            "Epoch 188: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0877 - val_loss: 1.7261\n",
            "Epoch 189/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0681\n",
            "Epoch 189: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0669 - val_loss: 1.7582\n",
            "Epoch 190/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0892\n",
            "Epoch 190: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0889 - val_loss: 1.7881\n",
            "Epoch 191/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0818\n",
            "Epoch 191: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0809 - val_loss: 1.7643\n",
            "Epoch 192/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0710\n",
            "Epoch 192: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0752 - val_loss: 1.7693\n",
            "Epoch 193/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0884\n",
            "Epoch 193: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0871 - val_loss: 1.7788\n",
            "Epoch 194/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0747\n",
            "Epoch 194: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0747 - val_loss: 1.8387\n",
            "Epoch 195/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0824\n",
            "Epoch 195: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0777 - val_loss: 1.8448\n",
            "Epoch 196/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.0802\n",
            "Epoch 196: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0766 - val_loss: 1.8208\n",
            "Epoch 197/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.0648\n",
            "Epoch 197: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0647 - val_loss: 1.8598\n",
            "Epoch 198/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0769\n",
            "Epoch 198: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0735 - val_loss: 1.8738\n",
            "Epoch 199/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0573\n",
            "Epoch 199: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0617 - val_loss: 1.9033\n",
            "Epoch 200/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0866\n",
            "Epoch 200: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0884 - val_loss: 1.8564\n",
            "Epoch 201/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.0820\n",
            "Epoch 201: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0825 - val_loss: 1.8694\n",
            "Epoch 202/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0989\n",
            "Epoch 202: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0986 - val_loss: 1.8116\n",
            "Epoch 203/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0836\n",
            "Epoch 203: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0822 - val_loss: 1.8330\n",
            "Epoch 204/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0711\n",
            "Epoch 204: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.0720 - val_loss: 1.8485\n",
            "Epoch 205/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0844\n",
            "Epoch 205: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.0848 - val_loss: 1.8639\n",
            "Epoch 206/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0779\n",
            "Epoch 206: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.0780 - val_loss: 1.8885\n",
            "Epoch 207/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0732\n",
            "Epoch 207: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.0719 - val_loss: 1.8967\n",
            "Epoch 208/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0834\n",
            "Epoch 208: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.0830 - val_loss: 1.8921\n",
            "Epoch 209/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.0829\n",
            "Epoch 209: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 14ms/step - loss: 0.0814 - val_loss: 1.8648\n",
            "Epoch 210/300\n",
            "19/24 [======================>.......] - ETA: 0s - loss: 0.0944\n",
            "Epoch 210: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 13ms/step - loss: 0.0902 - val_loss: 1.8456\n",
            "Epoch 211/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0836\n",
            "Epoch 211: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 13ms/step - loss: 0.0810 - val_loss: 1.8772\n",
            "Epoch 212/300\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.0764\n",
            "Epoch 212: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 15ms/step - loss: 0.0764 - val_loss: 1.8681\n",
            "Epoch 213/300\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.0883\n",
            "Epoch 213: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.0883 - val_loss: 1.8177\n",
            "Epoch 214/300\n",
            "19/24 [======================>.......] - ETA: 0s - loss: 0.0785\n",
            "Epoch 214: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.0772 - val_loss: 1.8694\n",
            "Epoch 215/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.0666\n",
            "Epoch 215: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 14ms/step - loss: 0.0662 - val_loss: 1.8625\n",
            "Epoch 216/300\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.0859\n",
            "Epoch 216: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.0859 - val_loss: 1.8635\n",
            "Epoch 217/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0751\n",
            "Epoch 217: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0716 - val_loss: 1.8819\n",
            "Epoch 218/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0717\n",
            "Epoch 218: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0733 - val_loss: 1.9198\n",
            "Epoch 219/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.0852\n",
            "Epoch 219: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0852 - val_loss: 1.9326\n",
            "Epoch 220/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0807\n",
            "Epoch 220: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.0797 - val_loss: 1.8700\n",
            "Epoch 221/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0723\n",
            "Epoch 221: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0743 - val_loss: 1.8932\n",
            "Epoch 222/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0828\n",
            "Epoch 222: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0826 - val_loss: 1.8722\n",
            "Epoch 223/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0842\n",
            "Epoch 223: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0893 - val_loss: 1.8774\n",
            "Epoch 224/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0722\n",
            "Epoch 224: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0716 - val_loss: 1.9093\n",
            "Epoch 225/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0566\n",
            "Epoch 225: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0588 - val_loss: 1.9602\n",
            "Epoch 226/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0725\n",
            "Epoch 226: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0693 - val_loss: 1.9356\n",
            "Epoch 227/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0648\n",
            "Epoch 227: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0672 - val_loss: 2.0175\n",
            "Epoch 228/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.0656\n",
            "Epoch 228: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0658 - val_loss: 1.9574\n",
            "Epoch 229/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0753\n",
            "Epoch 229: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0727 - val_loss: 1.8387\n",
            "Epoch 230/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0783\n",
            "Epoch 230: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0819 - val_loss: 1.8589\n",
            "Epoch 231/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.0764\n",
            "Epoch 231: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.0811 - val_loss: 1.8618\n",
            "Epoch 232/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0794\n",
            "Epoch 232: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0792 - val_loss: 1.8250\n",
            "Epoch 233/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.0683\n",
            "Epoch 233: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0662 - val_loss: 1.7828\n",
            "Epoch 234/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0511\n",
            "Epoch 234: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0517 - val_loss: 1.8652\n",
            "Epoch 235/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0670\n",
            "Epoch 235: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0731 - val_loss: 1.8424\n",
            "Epoch 236/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0777\n",
            "Epoch 236: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 9ms/step - loss: 0.0775 - val_loss: 1.8230\n",
            "Epoch 237/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0742\n",
            "Epoch 237: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0763 - val_loss: 1.8889\n",
            "Epoch 238/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0760\n",
            "Epoch 238: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0736 - val_loss: 1.8794\n",
            "Epoch 239/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0741\n",
            "Epoch 239: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0724 - val_loss: 1.8957\n",
            "Epoch 240/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0663\n",
            "Epoch 240: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0664 - val_loss: 1.8676\n",
            "Epoch 241/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0694\n",
            "Epoch 241: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0652 - val_loss: 1.8579\n",
            "Epoch 242/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0680\n",
            "Epoch 242: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0686 - val_loss: 1.9111\n",
            "Epoch 243/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0796\n",
            "Epoch 243: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0830 - val_loss: 1.9367\n",
            "Epoch 244/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0875\n",
            "Epoch 244: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0864 - val_loss: 1.8929\n",
            "Epoch 245/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0643\n",
            "Epoch 245: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0701 - val_loss: 1.8611\n",
            "Epoch 246/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0674\n",
            "Epoch 246: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0644 - val_loss: 1.8148\n",
            "Epoch 247/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0632\n",
            "Epoch 247: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0644 - val_loss: 1.8450\n",
            "Epoch 248/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.0680\n",
            "Epoch 248: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0704 - val_loss: 1.8645\n",
            "Epoch 249/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0780\n",
            "Epoch 249: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0822 - val_loss: 1.8765\n",
            "Epoch 250/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.0886\n",
            "Epoch 250: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.0839 - val_loss: 1.8408\n",
            "Epoch 251/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0707\n",
            "Epoch 251: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0696 - val_loss: 1.8206\n",
            "Epoch 252/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0594\n",
            "Epoch 252: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0603 - val_loss: 1.8619\n",
            "Epoch 253/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0517\n",
            "Epoch 253: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0532 - val_loss: 1.8359\n",
            "Epoch 254/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0652\n",
            "Epoch 254: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0658 - val_loss: 1.8935\n",
            "Epoch 255/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0540\n",
            "Epoch 255: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0539 - val_loss: 1.8607\n",
            "Epoch 256/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0534\n",
            "Epoch 256: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0537 - val_loss: 1.9155\n",
            "Epoch 257/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0590\n",
            "Epoch 257: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.0595 - val_loss: 1.9269\n",
            "Epoch 258/300\n",
            "19/24 [======================>.......] - ETA: 0s - loss: 0.0623\n",
            "Epoch 258: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.0595 - val_loss: 1.9761\n",
            "Epoch 259/300\n",
            "19/24 [======================>.......] - ETA: 0s - loss: 0.0592\n",
            "Epoch 259: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.0594 - val_loss: 1.9231\n",
            "Epoch 260/300\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.0664\n",
            "Epoch 260: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 12ms/step - loss: 0.0664 - val_loss: 1.9003\n",
            "Epoch 261/300\n",
            "19/24 [======================>.......] - ETA: 0s - loss: 0.0583\n",
            "Epoch 261: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 13ms/step - loss: 0.0571 - val_loss: 1.9217\n",
            "Epoch 262/300\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.0616\n",
            "Epoch 262: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 13ms/step - loss: 0.0616 - val_loss: 1.9801\n",
            "Epoch 263/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.0597\n",
            "Epoch 263: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 14ms/step - loss: 0.0596 - val_loss: 1.8924\n",
            "Epoch 264/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0698\n",
            "Epoch 264: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 13ms/step - loss: 0.0709 - val_loss: 1.9385\n",
            "Epoch 265/300\n",
            "23/24 [===========================>..] - ETA: 0s - loss: 0.0737\n",
            "Epoch 265: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 13ms/step - loss: 0.0722 - val_loss: 1.9408\n",
            "Epoch 266/300\n",
            "19/24 [======================>.......] - ETA: 0s - loss: 0.0640\n",
            "Epoch 266: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.0588 - val_loss: 1.9509\n",
            "Epoch 267/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.0626\n",
            "Epoch 267: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0661 - val_loss: 1.8998\n",
            "Epoch 268/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0375\n",
            "Epoch 268: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0410 - val_loss: 1.9166\n",
            "Epoch 269/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0552\n",
            "Epoch 269: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0557 - val_loss: 1.9630\n",
            "Epoch 270/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0542\n",
            "Epoch 270: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0605 - val_loss: 1.9708\n",
            "Epoch 271/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.0666\n",
            "Epoch 271: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0644 - val_loss: 2.0176\n",
            "Epoch 272/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.0470\n",
            "Epoch 272: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0517 - val_loss: 2.0782\n",
            "Epoch 273/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0596\n",
            "Epoch 273: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0596 - val_loss: 2.0821\n",
            "Epoch 274/300\n",
            "19/24 [======================>.......] - ETA: 0s - loss: 0.0718\n",
            "Epoch 274: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.0673 - val_loss: 2.0407\n",
            "Epoch 275/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.0731\n",
            "Epoch 275: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0746 - val_loss: 2.0774\n",
            "Epoch 276/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.0565\n",
            "Epoch 276: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0590 - val_loss: 1.9991\n",
            "Epoch 277/300\n",
            "19/24 [======================>.......] - ETA: 0s - loss: 0.0522\n",
            "Epoch 277: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.0583 - val_loss: 2.0549\n",
            "Epoch 278/300\n",
            "19/24 [======================>.......] - ETA: 0s - loss: 0.0623\n",
            "Epoch 278: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.0570 - val_loss: 2.0060\n",
            "Epoch 279/300\n",
            "19/24 [======================>.......] - ETA: 0s - loss: 0.0551\n",
            "Epoch 279: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.0540 - val_loss: 1.9773\n",
            "Epoch 280/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.0510\n",
            "Epoch 280: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0535 - val_loss: 2.0626\n",
            "Epoch 281/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.0607\n",
            "Epoch 281: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0597 - val_loss: 2.0101\n",
            "Epoch 282/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.0537\n",
            "Epoch 282: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0567 - val_loss: 2.0502\n",
            "Epoch 283/300\n",
            "19/24 [======================>.......] - ETA: 0s - loss: 0.0560\n",
            "Epoch 283: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.0569 - val_loss: 2.0175\n",
            "Epoch 284/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0600\n",
            "Epoch 284: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0615 - val_loss: 2.0826\n",
            "Epoch 285/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.0506\n",
            "Epoch 285: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0500 - val_loss: 2.0789\n",
            "Epoch 286/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.0576\n",
            "Epoch 286: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0581 - val_loss: 2.0345\n",
            "Epoch 287/300\n",
            "19/24 [======================>.......] - ETA: 0s - loss: 0.0467\n",
            "Epoch 287: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.0444 - val_loss: 2.0849\n",
            "Epoch 288/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0565\n",
            "Epoch 288: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0567 - val_loss: 2.0944\n",
            "Epoch 289/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.0545\n",
            "Epoch 289: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0550 - val_loss: 2.1299\n",
            "Epoch 290/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.0654\n",
            "Epoch 290: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0632 - val_loss: 2.1656\n",
            "Epoch 291/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0664\n",
            "Epoch 291: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0642 - val_loss: 2.0984\n",
            "Epoch 292/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0698\n",
            "Epoch 292: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0636 - val_loss: 2.0495\n",
            "Epoch 293/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.0619\n",
            "Epoch 293: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0646 - val_loss: 2.0481\n",
            "Epoch 294/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0573\n",
            "Epoch 294: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0562 - val_loss: 2.0439\n",
            "Epoch 295/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.0409\n",
            "Epoch 295: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 11ms/step - loss: 0.0473 - val_loss: 2.1343\n",
            "Epoch 296/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0450\n",
            "Epoch 296: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0450 - val_loss: 2.1574\n",
            "Epoch 297/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0482\n",
            "Epoch 297: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0511 - val_loss: 2.1368\n",
            "Epoch 298/300\n",
            "22/24 [==========================>...] - ETA: 0s - loss: 0.0482\n",
            "Epoch 298: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0503 - val_loss: 2.0776\n",
            "Epoch 299/300\n",
            "21/24 [=========================>....] - ETA: 0s - loss: 0.0605\n",
            "Epoch 299: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0616 - val_loss: 2.1065\n",
            "Epoch 300/300\n",
            "20/24 [========================>.....] - ETA: 0s - loss: 0.0614\n",
            "Epoch 300: val_loss did not improve from 1.02904\n",
            "24/24 [==============================] - 0s 10ms/step - loss: 0.0589 - val_loss: 2.0471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.models import load_model\n",
        "model = load_model('best_model.h5')"
      ],
      "metadata": {
        "id": "gEKJ2-mpmyOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "ind = np.random.randint(0,len(x_val)-1)\n",
        "\n",
        "random_music = x_val[ind]\n",
        "\n",
        "predictions=[]\n",
        "for i in range(10):\n",
        "\n",
        "    random_music = random_music.reshape(1,no_of_timesteps)\n",
        "\n",
        "    prob  = model.predict(random_music)[0]\n",
        "    y_pred= np.argmax(prob,axis=0)\n",
        "    predictions.append(y_pred)\n",
        "\n",
        "    random_music = np.insert(random_music[0],len(random_music[0]),y_pred)\n",
        "    random_music = random_music[1:]\n",
        "\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkJldrFtl1ju",
        "outputId": "92b3b002-c4b2-404c-cae7-53285da72062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 207ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "[7, 24, 25, 6, 7, 2, 18, 18, 7, 16]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_x))\n",
        "predicted_notes = [x_int_to_note[i] for i in predictions]"
      ],
      "metadata": {
        "id": "HiUg7S6jm3MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_midi(prediction_output):\n",
        "\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for pattern in prediction_output:\n",
        "\n",
        "        # pattern is a chord\n",
        "        if ('.' in pattern) or pattern.isdigit():\n",
        "            notes_in_chord = pattern.split('.')\n",
        "            notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "\n",
        "                cn=int(current_note)\n",
        "                new_note = note.Note(cn)\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                notes.append(new_note)\n",
        "\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "\n",
        "        # pattern is a note\n",
        "        else:\n",
        "\n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            output_notes.append(new_note)\n",
        "\n",
        "        # increase offset each iteration so that notes do not stack\n",
        "        offset += 1\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "    midi_stream.write('midi', fp='music.mid')\n"
      ],
      "metadata": {
        "id": "kW-lEAUbl5KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_to_midi(predicted_notes)"
      ],
      "metadata": {
        "id": "E_xFDwc9l91L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qrHoLTpAotz-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}